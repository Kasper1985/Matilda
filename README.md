# Matilda

Matilda is a simple AI assistant based on semantic kernel and using Azure OpenAI models as LLM in behind. This project was created as a Learn How-To to see the power of the semantic kernel and its possibilities. The assistant is in continuous development so more and more functionalities and features will come later.

## Prerequisites

To be able to run the assistant you will need following:

- An LLM model deployed by Azure or OpenAI.
- A connection to DB in case you want to store data in DB. Otherwise enough space on your disk or memory.
- Node.js installed on your computer to build the UI.
- .NET 8 or later to build the backend.

## Setup API

In order to compile the backend you need to install the latest .NET SDK which you can get [here](https://dotnet.microsoft.com/en-us/download/dotnet/8.0). After installing the SDK you can load the project and start with settings in appsettings.json

Here are following main sections to be filled:

- SemanticKernel
- Prompts
- ChatStore

All further provided information are based on the time this documentation was added / changed.

### SemanticKernel

In this section you are providing settings for the kernel and the model it is using to complete responses to user requests. First you need to provide the model provider. You have to options **AzureOpenAI** and **OpenAI**. Depending on which provider you choose you can use Azure Open AI models or Open AI model respectively. Depending on this configuration further settings are to setup.

For **AzureOpenAI** there are following options to set:

- Auth: Authorization type
- APIKey: The key provided by Azure Open AI for deployed models
- EndPoint: The endpoint to communicate with where the models are deployed
- Deployment: The name of deployment with needed model

For **OpenAI** there are following options to set:

- Auth: Authorization type
- APIKey: The key provided by Open AI for deployed models
- TextModel: Open AI model name to be used by semantic kernel

### Prompts

Here is the bound of all prompts used by sematic kernel. They are "prompt functions", system descriptions, and others. The main two prompts are:

- SystemDescriptions: This prompt will be added as the first channel to the chat, so the assistant gets initial behavior setup.
- InitialMessage: Is the second message that is default in the beginning of each chat addressed from assistant to the user.

### ChatStore

This section describes configurations for assistant storage. You have the possibility to store chat data in memory, on the disk or in data base. To configure this you need to provide the storage type in the **Type** property:

- Type: possible types are *InMemory*, *Filesystem* and *MongoDB*

For **InMemory** option you do not need any additional configurations. The API will store data in memory and read it from there. Of course by restarting of API all data will be lost.

For **Filesystem** you need to provide the path to the file, where the data will be stored. To achieve this you need to put following lines into the *ChatStore* section:

```json
...
"ChatStore": {
    "Type": "Filesystem",
    "Filesystem": {
        "FilePath": "chat-store.json"
    }
}
...
```

In this example the API will try to access the current run folder and create all needed files starting with the provided file name and extension. So to store chat messages the API will create "*chat-store.messages.json*" file and store there all messages generated by the assistant and sent by user.

For **MongoDB** option you need to provide the connection string to the DB and the name of a database where needed collections will be created and documented stored. The configuration should be like:

```json
...
"ChatStore": {
    "Type": "MongoDB",
    "MongoDB": {
        "ConnectionString": "mongodb://<user>:<password>@<server>:<port>?authSource=<database>",
        "DatabaseName": "<database>"
    }
}
...

```

Of course you need to replace *user*, *password*, *server*, *port* and *database* placeholder with your configurations.

It is no issue if you have more configurations for in that section. Depending on the chosen type only necessary options will be taken and others will be ignored.

Other configurations are pretty standard and do not needed to be explained.

## Setup UI

To run UI you need to have Node.js be installed on your computer. Otherwise you are unable to run NPM. The UI is written with [React](https://react.dev) and uses [Vite](https://vitejs.dev) as bundler. Also to create a native like apps for different OS the [Electron](https://electronjs.org) framework used to create such packages.

After download the code just run following command in your terminal:

```powershell
PS C:\Work> npm install
```

This will install all packages and modules needed to run UI. After installing run the next command to start the UI as well in browser as in electron

```powershell
PS C:\Work> npm run dev
```

This will compile the code and start server hosting the web site with UI. Moreover a new electron instance will be started and the UI will be loaded there. Alternative you can run your favorite browser and load the UI there by using following URL: http://localhost:3000

If you want to create a package to run the UI as a native app create the package with following command:

```powershell
PS C:\Work> npm run build
```

After finishing the process a new package will be created under the folder *publish* and you can start the application.

### UI settings

To configure the UI you need to provide the API to the application. You can do it in the *configuration.ts* file under *api* folder. Just replace the *SERVICE_BASE_URL* value with the path to the API and start the application.

## Final

Now you can start both API and UI and enjoy the power of AI.
